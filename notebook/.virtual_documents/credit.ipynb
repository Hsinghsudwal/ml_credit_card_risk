import pandas as pd
import numpy as np
import joblib
import time
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns



df=pd.read_csv('../data/data/credit.csv')
df.head()



df.info()



df.columns


df.describe(include='all')



df.isnull().sum()



df.nunique()





df['loan_status'].value_counts()


sns.barplot(x=df['person_home_ownership'].unique(), y=df['person_home_ownership'].value_counts())
plt.title('Housing')
plt.show()


sns.barplot(x=df['loan_grade'].unique(), y=df['loan_grade'].value_counts())
plt.title('Loan grade')
plt.show()



sns.barplot(x=df["person_income"],y=df["loan_intent"])
plt.title("Loan goals vs Person income")
plt.show()



sns.countplot(data = df, x = 'loan_grade', hue = 'loan_status')
plt.title(" loan grade vs loan status")
plt.show()



sns.countplot(data = df, x = 'person_home_ownership', hue = 'loan_status')
plt.title("home ownership vs loan status")
plt.show()



numerical_columns= df.select_dtypes(include=np.number)

for col in numerical_columns:
    sns.histplot( data=df , x=col , bins=30, color='orange')
    plt.show()



categorical_features= df.select_dtypes(include='object').columns.tolist()

for cat in categorical_features:
    sns.barplot( x=df[cat].value_counts().index, y= df[cat].value_counts().values)
    plt.show()


for i in numerical_columns:
    print(i)
    print(stats.describe(df[i]))
    print(stats.shapiro(df[i]))





df.duplicated().sum()


# drop with age >80 
df = df.drop(df[df['person_age'] > 80].index)


# new approch
data=df.copy()


df.isnull().sum()


df.dropna(axis=0,inplace=True)


df.head()


# Label encoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

for i in categorical_features:
    df[i] = le.fit_transform(df[i])

df.head()



plt.figure(figsize=(10,10))
sns.heatmap(np.round(df.corr(), 2),linewidths=0.1,vmax=1.0, 
            square=True,  linecolor='white', annot=True)
plt.show()






# Select features and target variable
X = df.drop(['loan_status'],axis = 1)
y =df['loan_status']


# initial modeling
from sklearn.preprocessing import StandardScaler
scaler =  StandardScaler()
Xscale  = scaler.fit_transform(X)
Xscale
#test_scaled = scaler.transform(X_test)


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(Xscale, y, test_size = 0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape


from imblearn.over_sampling import SMOTE

smt = SMOTE()
X_res, y_res = smt.fit_resample(X_train, y_train)
print(f'after train split: ,{X_res.shape, y_res.shape}')

X_res, y_res = smt.fit_resample(Xscale, y)
print(f'before train split after scale: ,{X_res.shape, y_res.shape}')

X_res, y_res = smt.fit_resample(X, y)
print(f'before scale: ,{X_res.shape, y_res.shape}')


# Preprocessing, modelling and evaluating
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, classification_report, f1_score,precision_score,recall_score, roc_auc_score
from sklearn.metrics import make_scorer, RocCurveDisplay, ConfusionMatrixDisplay
from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold
from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict 
from sklearn.model_selection import cross_validate, GridSearchCV
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression,LinearRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from xgboost import XGBClassifier
import lightgbm as lgb


lr = LinearRegression().fit(X_train,y_train)
y_pred=lr.predict(X_test)
print("LinearRegression:")
print(f'training accuracy:{lr.score(X_train,y_train)}')
print(f'Testing accuracy:{lr.score(X_test,y_test)}')



def evaluate_clf(true, predicted):
    acc = accuracy_score(true, predicted) # Calculate Accuracy
    f1 = f1_score(true, predicted) # Calculate F1-score
    precision = precision_score(true, predicted) # Calculate Precision
    recall = recall_score(true, predicted)  # Calculate Recall
    roc_auc = roc_auc_score(true, predicted) #Calculate Roc
    return acc, f1 , precision, recall, roc_auc


# !pip install lightgbm


models = {
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "K-Neighbors Classifier": KNeighborsClassifier(),
    "XGBClassifier": XGBClassifier(),
    "LogisticRegression": LogisticRegression(),
    "Support Vector Classifier": SVC(gamma='auto'),
    # "AdaBoost Classifier": AdaBoostClassifier(),
    "Lightgbm Classifier": lgb.LGBMClassifier()

}


# Create a function which can evaluate models and return a report 
def evaluate_models(X, y, models):
    '''
    This function takes in X and y and models dictionary as input
    It splits the data into Train Test split
    Iterates through the given model dictionary and evaluates the metrics
    Returns: Dataframe which contains report of all models metrics with cost
    '''
    # separate dataset into train and test
    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)
    
    models_list = []
    accuracy_list = []
    auc= []
    
    for i in range(len(list(models))):
        model = list(models.values())[i]
        model.fit(X_train, y_train) # Train model

        # Make predictions
        y_train_pred = model.predict(X_train)
        y_test_pred = model.predict(X_test)

        # Training set performance
        model_train_accuracy, model_train_f1,model_train_precision,\
        model_train_recall,model_train_rocauc_score=evaluate_clf(y_train ,y_train_pred)


        # Test set performance
        model_test_accuracy,model_test_f1,model_test_precision,\
        model_test_recall,model_test_rocauc_score=evaluate_clf(y_test, y_test_pred)

        print(list(models.keys())[i])
        models_list.append(list(models.keys())[i])

        print('Model performance for Training set')
        print("- Accuracy: {:.4f}".format(model_train_accuracy))
        print('- F1 score: {:.4f}'.format(model_train_f1)) 
        print('- Precision: {:.4f}'.format(model_train_precision))
        print('- Recall: {:.4f}'.format(model_train_recall))
        print('- Roc Auc Score: {:.4f}'.format(model_train_rocauc_score))

        print('----------------------------------')

        print('Model performance for Test set')
        print('- Accuracy: {:.4f}'.format(model_test_accuracy))
        accuracy_list.append(model_test_accuracy)
        print('- F1 score: {:.4f}'.format(model_test_f1))
        print('- Precision: {:.4f}'.format(model_test_precision))
        print('- Recall: {:.4f}'.format(model_test_recall))
        print('- Roc Auc Score: {:.4f}'.format(model_test_rocauc_score))
        auc.append(model_test_rocauc_score)
        print('='*35)
        print('\n')
        
    report=pd.DataFrame(list(zip(models_list, accuracy_list)), columns=['Model Name', 'Accuracy']).sort_values(by=['Accuracy'], ascending=False)
        
    return report


# Pass raw scale data
model_report =evaluate_models(X=Xscale, y=y, models=models)


# Result models
model_report





# Hypertunning Random_search and Grid_Search
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold

kf = KFold(n_splits=30)


xgboost_params = {
    'max_depth': range(2, 10, 2),
    'min_child_weight':range (1, 10, 2)

}
lgbm_params={
    'n_estimators': range(1,10,1),
    'learning_rate': [0.5,1],
    'max_bin': [255]
}

grid_models = [
    ('XGBoost', XGBClassifier(), xgboost_params),
    ("LGBM", lgb.LGBMClassifier(), lgbm_params)
]

model_param = {}
for name, model, param_grid in grid_models:
    print(name, model, param_grid)
    gsc = GridSearchCV(estimator=model,
                                param_grid=param_grid,
                                   cv=kf,
                                   verbose=2, 
                                   n_jobs=-1)
    gsc.fit(X_train, y_train)
    model_param[name] = gsc.best_params_

for model_name in model_param:
    print(f"---------------- Best Params for {model_name} -------------------")
    print(model_param[model_name])


model_param


# Testing with different dataset
from sklearn.metrics import roc_auc_score,roc_curve
best_models = {
    "XGBClassifier": XGBClassifier(**model_param['XGBoost'],n_jobs=-1),
    "LGBMClassifier": lgb.LGBMClassifier(**model_param['LGBM'])
}
tuned_report =evaluate_models(X=Xscale, y=y, models=best_models)


tuned_report


# params = {'min_child_weight': 4, 'max_depth': 6}

best_model = XGBClassifier(**model_param['XGBoost'])
# best_model = XGBClassifier(**model_param['XGBoost'])
best_model = best_model.fit(X_train,y_train)
y_pred_best = best_model.predict(X_test)
test_score = accuracy_score(y_test,y_pred_best)
cr = classification_report(y_test,y_pred_best)

print("FINAL MODEL 'XGBoost'")
print(f'Training Accuracy: {best_model.score(X_train,y_train)}')
print("Testing Accuracy Score: {:.4f}".format(test_score))
print (cr)



# ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test)


from matplotlib.pyplot import figure
feat_importances = pd.Series(best_model.feature_importances_, index = X.columns).sort_values(ascending = True)
feat_importances.plot(kind = 'barh')


# Save best xgboost model
import joblib
import os

print(f"Best model: {best_model}")
joblib.dump(best_model, 'model.pkl')





path = open('model.pkl','rb')
model = joblib.load(path)


df.iloc[300]


trys=X.iloc[300].values.reshape(1,-1)
trys


est1=scaler.transform(trys)
est1
# data=[[21,   0, 630,   1,  13,   4,   2,   1,   1,   2,   2,   1,   2,
#          1,   1,   2,   0,   0,   2,   2]]


model.predict(est1)[0]


# !pip install xgboost
# !pip install hyperopt


from xgboost import XGBClassifier
import hyperopt
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials
import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report,ConfusionMatrixDisplay, \
                            precision_score, recall_score, f1_score, roc_auc_score, \
                            roc_curve,confusion_matrix


space={'max_depth': hp.quniform("max_depth", 3, 18, 1),
        'gamma': hp.uniform ('gamma', 1,9),
        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),
        'reg_lambda' : hp.uniform('reg_lambda', 0,1),
        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),
        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),
        'n_estimators': 180
    }


def hyperparameter_tuning(space):
    model = xgb.XGBClassifier(n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],
                         reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),
                         colsample_bytree=int(space['colsample_bytree']))
    # evaluation = [( X_train, y_train), ( X_test, y_test)]
    
    model.fit(X_train, y_train)
            # eval_set=evaluation, eval_metric="rmse",
            # early_stopping_rounds=10,verbose=False)

    pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, pred)
    print ("SCORE:", accuracy)
    #change the metric if you like
    return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}


trials = Trials()
best = fmin(fn=hyperparameter_tuning,
            space=space,
            algo=tpe.suggest,
            max_evals=100,
            trials=trials)


print(best)





data.head()


data.columns


# Building pipeline
# from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

x = data.drop(['loan_status'],axis = 1)
y = data['loan_status']

num_features = x.select_dtypes(include=['int64', 'float64']).columns
cat_features = x.select_dtypes(include=['object']).columns

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

num_transformer = Pipeline([
    ('imputer',SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

cat_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', num_transformer, num_features),
    ('cat', cat_transformer, cat_features)
])


# X_train_transformed = preprocessor.fit_transform(X_train)
# X_test_transformed = preprocessor.transform(X_test)
clf= XGBClassifier(**model_param['XGBoost'])

pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('smote', SMOTE()),  # Apply SMOTE for oversampling
        ('classifier', clf)  # Use the current model
        ])

# Fit the pipeline
pipeline.fit(X_train, y_train)
ypred = pipeline.predict(X_test)
test_score = accuracy_score(y_test,ypred)
cr = classification_report(y_test,ypred)

print("FINAL MODEL")
print("Training Accuracy Score: {:.4f}".format(pipeline.score(X_train,y_train)))
print("Testing Accuracy Score: {:.4f}".format(test_score))
print (cr)


# without smote
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
# from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

x = data.drop(['loan_status'],axis = 1)
y = data['loan_status']

num_features = x.select_dtypes(include=['int64', 'float64']).columns
cat_features = x.select_dtypes(include=['object']).columns

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

num_transformer = Pipeline([
    ('imputer',SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

cat_transformer = Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer([
    ('num', num_transformer, num_features),
    ('cat', cat_transformer, cat_features)
])


# X_train_transformed = preprocessor.fit_transform(X_train)
# X_test_transformed = preprocessor.transform(X_test)
clf= XGBClassifier(**model_param['XGBoost'])

pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        # ('smote', SMOTE()),  # Apply SMOTE for oversampling
        ('classifier', clf)  # Use the current model
        ])

# Fit the pipeline
pipeline.fit(X_train, y_train)
ypred = pipeline.predict(X_test)
test_score = accuracy_score(y_test,ypred)
cr = classification_report(y_test,ypred)

print("FINAL MODEL")
print("Training Accuracy Score: {:.4f}".format(pipeline.score(X_train,y_train)))
print("Testing Accuracy Score: {:.4f}".format(test_score))
print (cr)



